{'title': 'BIOCLIP: A Vision Foundation Model for the Tree of Life',
 'authors': [{'name': 'Samuel Stevens',
   'affiliation': 'The Ohio State University'},
  {'name': 'Jiaman Wu', 'affiliation': 'The Ohio State University'},
  {'name': 'Matthew J Thompson', 'affiliation': 'The Ohio State University'},
  {'name': 'Elizabeth G Campolongo',
   'affiliation': 'The Ohio State University'},
  {'name': 'Chan Hee Song', 'affiliation': 'The Ohio State University'},
  {'name': 'David Edward Carlyn', 'affiliation': 'The Ohio State University'},
  {'name': 'Li Dong', 'affiliation': 'Microsoft Research'},
  {'name': 'Wasila M Dahdul',
   'affiliation': 'University of California, Irvine'},
  {'name': 'Charles Stewart',
   'affiliation': 'Rensselaer Polytechnic Institute'},
  {'name': 'Tanya Berger-Wolf', 'affiliation': 'The Ohio State University'},
  {'name': 'Wei-Lun Chao', 'affiliation': 'The Ohio State University'},
  {'name': 'Yu Su', 'affiliation': 'The Ohio State University'}],
 'abstract': 'Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general organismal biology questions on images is of timely need. To approach this, we curate and release TREEOFLIFE-10M, the largest and most diverse ML-ready dataset of biology images. We then develop BIOCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TREEOFLIFE-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on diverse fine-grained biology classification tasks and find that BIOCLIP consistently and substantially outperforms existing baselines (by 16% to 17% absolute). Intrinsic evaluation reveals that BIOCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability.',
 'key_findings': "BIOCLIP consistently and substantially outperforms existing baselines (by 16% to 17% absolute) on diverse fine-grained biology classification tasks. Intrinsic evaluation reveals that BIOCLIP has learned a hierarchical representation conforming to the tree of life. The introduction of TREEOFLIFE-10M, a large-scale, diverse ML-ready biology image dataset with over 10 million images covering 454 thousand taxa, and BIOCLIP, a vision foundation model for the tree of life, trained with suitable use of taxa in TREEOFLIFE-10M. These tools address the need for suitable pre-training datasets and strategies in the field of evolutionary biology and ecology, where existing general-domain vision models fall short. BIOCLIP substantially outperforms both CLIP and OpenCLIP in zero-shot and few-shot settings with an average absolute improvement of 17% (zero-shot) and 16% (few-shot). BIOCLIP has learned a more fine-grained hierarchical representation conforming to the tree of life, explaining its superior generalization. TREEOFLIFE-10M dataset contains over 10M images across more than 450K unique taxonomic names. The datasets TREEOFLIFE-10M and RARE SPECIES are available on Hugging Face under a public domain waiver, including CSVs with image metadata and links to the primary sources, accompanied by a GitHub repository with scripts to generate the datasets. BIOCLIP is initialized from OpenAI’s public CLIP checkpoint and continually pre-trained on TREEOFLIFE-10M with CLIP’s multimodal contrastive learning objective. The key realization of this work is that the multimodal contrastive learning objective used in CLIP can be repurposed for leveraging the hierarchical structure of the label space, significantly improving generalization for learning hierarchical representations conforming to a taxonomy. The mixed text type training strategy retains the generalization benefits of taxonomic names while providing more flexibility in using other names at inference time. BIOCLIP, trained on TREEOFLIFE-10M, shows improved performance over general vision models and demonstrates effectiveness in a variety of biologically-relevant classification tasks across all four multi-celled kingdoms in the tree of life. BIOCLIP can leverage taxonomic names and is tested for generalization to unseen taxa, specifically in the context of classifying rare species. A new evaluation task, RARE SPECIES, is introduced to assess BIOCLIP's performance in classifying species classified as Near Threatened, Vulnerable, Endangered, Critically Endangered, or Extinct in the Wild by the IUCN Red List. Approximately 25K species on the IUCN Red List are considered, and 400 species represented by at least 30 images in the EOL dataset are selected for this task. BIOCLIP substantially outperforms both baseline CLIP models and the iNat21-trained CLIP model at zero-shot classification, especially on unseen taxa. BIOCLIP's strong zero-shot performance on a broad and diverse set of tasks is attributed to the broad and diverse classes in TREEOFLIFE-10M. The study demonstrates BIOCLIP's out-of-distribution generalization to unseen taxa and its potential applications in addressing the ongoing biodiversity crisis. The zero-shot ablation results show several salient observations. First, using a mixed text type strategy for training, which involves using different captions for each image, significantly affects the performance. The study demonstrates the effectiveness of the CLIP objective for pre-training on a labeled image dataset, particularly in few-shot settings, outperforming both cross-entropy and hierarchical cross-entropy baselines significantly. It also highlights the importance of incorporating taxonomic structure for generalization and shows that using mixed text types for training enhances performance across different text types at test time. Additionally, BIOCLIP's ability to classify beyond species, such as plant diseases, in both zero-shot and few-shot settings is confirmed, indicating its utility for tasks requiring classification of both species and disease. The intrinsic evaluation reveals that BIOCLIP learns useful visual representations that align with taxonomic hierarchies. At higher ranks like kingdom and phylum, both CLIP and BIOCLIP have good separation, but BIOCLIP’s representations are more fine-grained and contain a richer clustering structure. At lower ranks, BIOCLIP produces evidently more separable features, while CLIP’s features are cluttered and lack a clear structure. BIOCLIP is a strong fine-grained classifier for biology in both zero- and few-shot settings. Using the entire taxonomic name leads to stronger generalization than other caption types. BIOCLIP-embedded images better match the taxonomic hierarchy.",
 'limitation_of_sota': "Most computational methods and tools for extracting biologically relevant information from images are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. Existing general-domain vision models trained on hundreds of millions of images fall short when applied to evolutionary biology and ecology, particularly for fine-grained comparisons such as between Onoclea sensibilis and Onoclea hintonii. Current datasets lack either scale, diversity, or fine-grained labels, and mainstream pre-training algorithms insufficiently consider the tree of life taxonomy. This fails to recognize and leverage the rich structure of taxonomic labels—taxa do not exist in isolation but are interconnected in a comprehensive taxonomy. Consequently, a model trained via plain supervised classification may not generalize well to taxa unseen in training, nor could it support zero-shot classification of unseen taxa. Existing CLIP models like CLIP and OpenCLIP are limited in their ability to generalize across the diverse and fine-grained categories found in biological data. One of the most salient differences for the biology domain is its rich label space, with 2M+ recorded species as of 2022, connected in a hierarchical taxonomy, posing a challenge for training a foundation model that can achieve satisfactory coverage and generalization. Many vision foundation models, such as ResNet and Swin Transformer, adopt a supervised classification objective and directly learn the mapping from input images to class indices, treating each class label as a distinct symbol and neglecting their relationships. Existing models like OpenAI's CLIP and OpenCLIP, though effective with common names due to their pervasive presence in training data, lack the flexibility and specificity required for scientific and taxonomic name-based inference in biological classification tasks. Existing models like CLIP and OpenCLIP work best with common names and have limitations in leveraging taxonomic names for classification tasks. Existing models struggle with zero-shot classification, particularly in accurately identifying rare or unseen species. Existing models struggle with generalization across different text types and may not effectively incorporate taxonomic structures for classification tasks. Additionally, they may not be optimized for few-shot learning scenarios or capable of classifying beyond species level, such as identifying diseases based on visual symptoms. Further work finds that dataset diversity and better alignment between the image and caption semantics are more important than dataset size and lead to stronger performance on downstream tasks. Prior work applied hierarchies to smaller label spaces and did not efficiently learn visual representations over extensive taxa.",
 'proposed_solution': "The development of BIOCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TREEOFLIFE-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. The proposed solution emphasizes the importance of a model's ability to generalize to taxa not present in training data, learn fine-grained representations of images of organisms, and perform strongly despite the high cost of data collection and labeling in biology. This approach is crucial for dealing with the vast diversity of life, which includes organisms that are visually similar or mimic others for survival advantages. The proposed solution includes the creation of TREEOFLIFE-10M, the largest ML-ready dataset to-date of biology images with associated taxonomic labels, and BIOCLIP, a vision foundation model for the tree of life. BIOCLIP is trained using a suitable pre-training strategy that leverages the special properties of the biology domain, such as the tree of life taxonomy, to better achieve generalization, fine-grained classification, and data efficiency. We propose a novel strategy combining CLIP-style multimodal contrastive learning with the rich biological taxonomy for BIOCLIP. We 'flatten' the taxonomy from Kingdom to the distal-most taxon rank into a string called taxonomic name, and use the CLIP contrastive learning objective to learn to match images with their corresponding taxonomic names. Intuitively, this helps the model generalize to unseen taxa—even if the model has not seen a species, it has likely learned a reasonable representation for that species’ genus or family. BIOCLIP also supports zero-shot classification with taxonomic names of unseen taxa. We further propose, and demonstrate the effectiveness of, a mixed text type training strategy; by mixing different text types (e.g., taxonomic vs. scientific vs. common names) during training, we retain the generalization from taxonomic names while being more flexibility at test time. The TREEOFLIFE-10M dataset was curated, integrating iNat21, EOL dataset, and BIOSCAN-1M, to create the most diverse large-scale public ML-ready dataset for computer vision models in biology. This dataset aims to address the limitations of existing datasets by providing a more diverse and comprehensive collection of biology images, focusing on species diversity and including a significant number of images from underrepresented categories such as insects. The proposed solution involves using BIOCLIP, initialized from OpenAI’s public CLIP checkpoint and continually pre-trained on TREEOFLIFE-10M with CLIP’s multimodal contrastive learning objective, to address the challenge of a rich label space in the biology domain. This work proposes repurposing CLIP’s multimodal contrastive learning objective for learning hierarchical representations conforming to a taxonomy. The approach involves training two uni-modal embedding models, a vision encoder and a text encoder, to maximize feature similarity between positive (image, text) pairs and minimize feature similarity between negative (image, text) pairs, embedding the taxonomic hierarchy into a dense label space. In the context of training B IOCLIP, the proposed solution involves two main objectives: (1) maximize feature similarity between positive (image, text) pairs and (2) minimize feature similarity between negative (image, text) pairs. Positive pairs are from the training data, and negative pairs are all other possible (image, text) pairings in a batch. After training, CLIP’s encoder models embed individual instances of their respective modalities into a shared feature space. The text input to CLIP is formatted to incorporate the taxonomic structure by considering the taxonomic name, which concatenates all labels from root to leaf into a single string for each species. BIOCLIP is proposed with a mixed text type training strategy, utilizing a combination of scientific names, common names, and other text types for training. This approach is aimed at improving flexibility and performance in biological image classification tasks, especially when only one type of label is available. The model is pre-trained on the TREEOFLIFE-10M dataset with OpenAI's CLIP weights and further evaluated on various classification tasks to demonstrate its efficacy. BIOCLIP is designed to leverage both taxonomic and common names for classification tasks, aiming to generalize well to unseen taxa without the need for re-training for every new species. This approach is particularly useful for classifying rare species, an important application in the context of global conservation efforts. BIOCLIP, trained on a diverse dataset from TREEOFLIFE-10M, shows significant improvement in zero-shot classification accuracy across a variety of taxa, including rare species. This model leverages the diversity of the training data to achieve broad and useful image representations for classification tasks. The proposed solution involves using a mixed text type strategy for training, which includes using different captions for each image rather than a single fixed caption. This approach is based on prior works that find the diversity of captions makes stronger vision models. The study proposes using the CLIP objective for pre-training on the TREEOFLIFE-1M dataset, which outperforms traditional cross-entropy and hierarchical cross-entropy methods in few-shot classification tasks. It emphasizes the importance of mixed text type pre-training for maintaining generalization benefits and flexibility across different text types. Furthermore, the study introduces BIOCLIP, which leverages the CLIP objective to classify both species and diseases, demonstrating its effectiveness in few-shot settings and its ability to learn taxonomic hierarchies. TREEOFLIFE-10M emphasizes the importance of diversity, adding over 440K classes to iNat21’s 10K and leading to stronger zero-shot performance. Domain-specific training often beats general training, but subject-matter experts are often too expensive to hire to label large-scale domain-specific datasets. Image-text training is thus particularly potent because models can learn from noisy image-text pairs. Introducing TREEOFLIFE-10M and BIOCLIP, a large-scale diverse biology image dataset and a foundation model for the tree of life, respectively, which applies hierarchical classification to 454K unique classes through a repurposed CLIP objective.",
 'paper_limitations': "The label granularity in BIOSCAN-1M may still be limited for insects, with a significant percentage of images labeled only to the family level and not to the more specific genus or species levels. Taxonomic hierarchies are notoriously noisy and rarely consistent between sources, which poses challenges for label aggregation and standardization. The paper does not explicitly mention its limitations, but potential limitations could include the reliance on the availability and quality of taxonomic and common name data, the generalizability of the model to unseen species, and the computational resources required for training on large datasets like TREEOFLIFE-10M. The paper does not specify the limitations of the proposed BIOCLIP solution directly, but it implies that the challenge of classifying rare species is significant due to the lack of a diverse, publicly available dataset for rare species classification. The study was conducted with computational constraints, limiting the training to only a 10% subset of TREEOFLIFE-10M. Additionally, the impact of different text types on generalization was explored, but the diversity of captions and their specific contributions to the strength of the vision models could be further detailed. The paper does not specify its limitations directly, but potential limitations could include the reliance on the specific dataset (TREEOFLIFE-1M) for training, which may not be representative of all biodiversity. Additionally, the effectiveness of the CLIP objective and BIOCLIP in broader or more complex classification tasks beyond the scope of the study remains to be fully explored. BIOCLIP is fundamentally trained to do classification. Future work will scale up the data and collect richer textual descriptions of species' appearances for extracting fine-grained trait-level representations."}