{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33771720",
   "metadata": {},
   "source": [
    "# ðŸš€ iText2KG + FalkorDB Quickstart Guide\n",
    "\n",
    "This notebook demonstrates how to use **iText2KG** with **FalkorDB** for building and visualizing knowledge graphs from text documents.\n",
    "\n",
    "## Features Covered:\n",
    "- Document distillation with custom schemas\n",
    "- Knowledge graph construction using iText2KG_Star (recommended)\n",
    "- Complete FalkorDB integration with advanced features\n",
    "- Graph statistics and analytics\n",
    "- Error handling and fallback mechanisms\n",
    "\n",
    "**Updated:** August 2025 with full FalkorDB integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce00a6",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install itext2kg falkordb langchain_openai\n",
    "```\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.9+\n",
    "- OpenAI API key\n",
    "- FalkorDB server running (default: localhost:6379)\n",
    "\n",
    "**FalkorDB Setup:**\n",
    "```bash\n",
    "# Using Docker (recommended)\n",
    "docker run -p 6379:6379 -it --rm falkordb/falkordb:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ac07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# iText2KG imports\n",
    "from itext2kg import DocumentDistiller, iText2KG_Star, FalkorDBStorage\n",
    "from itext2kg.logging_config import setup_logging, get_logger\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(level=\"INFO\")\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fd014",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configuration\n",
    "\n",
    "Set up your API keys and database connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting iText2KG + FalkorDB Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# OpenAI Configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or input('Enter your OpenAI API key: ')\n",
    "\n",
    "# Initialize LLM and Embeddings\n",
    "llm_model = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# FalkorDB Configuration\n",
    "FALKORDB_CONFIG = {\n",
    "    \"host\": os.getenv(\"FALKORDB_HOST\", \"localhost\"),\n",
    "    \"port\": int(os.getenv(\"FALKORDB_PORT\", 6379)),\n",
    "    \"password\": os.getenv(\"FALKORDB_PASSWORD\", None),\n",
    "    \"graph_name\": \"NewsGraph\"\n",
    "}\n",
    "\n",
    "print(f\"ðŸ”§ Configuration complete!\")\n",
    "print(f\"FalkorDB: {FALKORDB_CONFIG['host']}:{FALKORDB_CONFIG['port']}\")\n",
    "print(f\"LLM Model: {llm_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b864c56",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Define Data Schema\n",
    "\n",
    "Create a Pydantic schema for structured information extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3dce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsArticle(BaseModel):\n",
    "    \"\"\"Schema for extracting structured information from news articles.\"\"\"\n",
    "    title: str = Field(default=\"\", description=\"The title of the article\")\n",
    "    companies: List[str] = Field(default_factory=list, description=\"Companies mentioned in the article\")\n",
    "    people: List[str] = Field(default_factory=list, description=\"People mentioned in the article\")\n",
    "    locations: List[str] = Field(default_factory=list, description=\"Locations mentioned in the article\")\n",
    "    key_events: str = Field(default=\"\", description=\"Main events described in the article\")\n",
    "    technologies: List[str] = Field(default_factory=list, description=\"Technologies or products mentioned\")\n",
    "    funding_info: str = Field(default=\"\", description=\"Any funding or financial information mentioned\")\n",
    "\n",
    "print(\"ðŸ“‹ NewsArticle schema defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9769eb",
   "metadata": {},
   "source": [
    "## ðŸ“° Sample Data\n",
    "\n",
    "Let's use a sample news article about a tech acquisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef36581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample article text\n",
    "article_text = \"\"\"\n",
    "Apple Inc. announced today that it has acquired Emotient, a San Diego-based artificial\n",
    "intelligence startup specializing in facial expression recognition technology. The acquisition\n",
    "was led by Apple's Senior Vice President of Software Engineering, Craig Federighi.\n",
    "\n",
    "Emotient's technology will be integrated into Apple's machine learning initiatives. The startup\n",
    "was founded in 2012 by Dr. Ken Denman and has raised $8 million in funding from venture capital firms.\n",
    "\n",
    "The acquisition represents Apple's continued investment in artificial intelligence and machine\n",
    "learning capabilities. Industry analysts believe this technology could be integrated into future\n",
    "iPhone and iPad applications for emotion recognition and user experience enhancement.\n",
    "\n",
    "Tim Cook, Apple's CEO, stated that this acquisition aligns with the company's strategy to develop\n",
    "more intelligent and intuitive user interfaces. The Emotient team will join Apple's AI research\n",
    "division in Cupertino, California.\n",
    "\"\"\"\n",
    "\n",
    "# Information extraction query\n",
    "extraction_query = \"\"\"\n",
    "# DIRECTIVES:\n",
    "- Act like a professional business news analyst\n",
    "- Extract key information from this business article\n",
    "- Focus on factual information only\n",
    "- If information is not found, leave it empty\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Article loaded ({len(article_text)} characters)\")\n",
    "print(f\"Extraction query prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a62ec",
   "metadata": {},
   "source": [
    "## ðŸ” Step 1: Document Distillation\n",
    "\n",
    "Extract structured information from the raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161796fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def distill_document():\n",
    "    \"\"\"Extract structured information from the article.\"\"\"\n",
    "    \n",
    "    logger.info(\"Starting document distillation...\")\n",
    "    \n",
    "    # Initialize document distiller\n",
    "    distiller = DocumentDistiller(llm_model=llm_model)\n",
    "    \n",
    "    # Extract structured information\n",
    "    distilled = await distiller.distill(\n",
    "        documents=[article_text],\n",
    "        IE_query=extraction_query,\n",
    "        output_data_structure=NewsArticle\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    doc_dict = (\n",
    "        distilled.model_dump() \n",
    "        if hasattr(distilled, 'model_dump') \n",
    "        else distilled.dict()\n",
    "    )\n",
    "    \n",
    "    # Create semantic blocks\n",
    "    semantic_blocks = [\n",
    "        f\"{k}: {', '.join(v) if isinstance(v, list) else v}\"\n",
    "        for k, v in doc_dict.items() if v\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Generated {len(semantic_blocks)} semantic blocks\")\n",
    "    \n",
    "    return doc_dict, semantic_blocks\n",
    "\n",
    "# Run distillation\n",
    "doc_data, semantic_blocks = await distill_document()\n",
    "\n",
    "print(\"\\nExtracted Information:\")\n",
    "for key, value in doc_data.items():\n",
    "    if value:\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(semantic_blocks)} semantic blocks for KG construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce66fa",
   "metadata": {},
   "source": [
    "## ðŸ•¸ï¸ Step 2: Knowledge Graph Construction\n",
    "\n",
    "Build a knowledge graph using iText2KG_Star (recommended approach):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a05d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def build_knowledge_graph(sections):\n",
    "    \"\"\"Build knowledge graph from semantic blocks.\"\"\"\n",
    "    \n",
    "    logger.info(\"Building knowledge graph with iText2KG_Star...\")\n",
    "    \n",
    "    # Initialize iText2KG_Star\n",
    "    itext2kg_star = iText2KG_Star(\n",
    "        llm_model=llm_model, \n",
    "        embeddings_model=embeddings_model\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Build the knowledge graph\n",
    "        knowledge_graph = await itext2kg_star.build_graph(\n",
    "            sections=sections,\n",
    "            ent_threshold=0.7,      # Entity similarity threshold\n",
    "            rel_threshold=0.7,      # Relationship similarity threshold\n",
    "            max_tries=3,            # Max attempts for extraction\n",
    "            entity_name_weight=0.6, # Weight for entity name in matching\n",
    "            entity_label_weight=0.4, # Weight for entity label in matching\n",
    "            observation_date=\"2025-08-05\"  # Optional: temporal context\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Built KG: {len(knowledge_graph.entities)} entities, {len(knowledge_graph.relationships)} relationships\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"Extraction failed: {e}. Using fallback data...\")\n",
    "        \n",
    "        # Fallback: simple test sections\n",
    "        fallback_sections = [\n",
    "            \"Apple Inc. is a technology company based in Cupertino, California.\",\n",
    "            \"Tim Cook is the CEO of Apple Inc.\",\n",
    "            \"Craig Federighi is the Senior Vice President of Apple Inc.\",\n",
    "            \"Emotient is an AI startup specializing in facial recognition.\",\n",
    "            \"Apple Inc. acquired Emotient in 2016.\",\n",
    "            \"Dr. Ken Denman founded Emotient in 2012.\",\n",
    "            \"Emotient raised $8 million in funding.\"\n",
    "        ]\n",
    "        \n",
    "        knowledge_graph = await itext2kg_star.build_graph(\n",
    "            sections=fallback_sections,\n",
    "            ent_threshold=0.7,\n",
    "            rel_threshold=0.7,\n",
    "            max_tries=1\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Fallback KG: {len(knowledge_graph.entities)} entities, {len(knowledge_graph.relationships)} relationships\")\n",
    "    \n",
    "    return knowledge_graph\n",
    "\n",
    "# Build knowledge graph\n",
    "kg = await build_knowledge_graph(semantic_blocks)\n",
    "\n",
    "print(\"\\nðŸ•¸ï¸ Knowledge Graph Summary:\")\n",
    "print(f\"  â€¢ Entities: {len(kg.entities)}\")\n",
    "print(f\"  â€¢ Relationships: {len(kg.relationships)}\")\n",
    "\n",
    "print(\"\\nEntities by Type:\")\n",
    "entity_types = {}\n",
    "for entity in kg.entities:\n",
    "    entity_types[entity.label] = entity_types.get(entity.label, 0) + 1\n",
    "    \n",
    "for label, count in sorted(entity_types.items()):\n",
    "    print(f\"  â€¢ {label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb5ffc",
   "metadata": {},
   "source": [
    "## ðŸ—„ï¸ Step 3: FalkorDB Integration\n",
    "\n",
    "Store and visualize the knowledge graph in FalkorDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98516438",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def integrate_with_falkordb(knowledge_graph):\n",
    "    \"\"\"Store knowledge graph in FalkorDB with enhanced features.\"\"\"\n",
    "    \n",
    "    logger.info(\"Connecting to FalkorDB...\")\n",
    "    \n",
    "    # Initialize FalkorDB storage\n",
    "    fdb = FalkorDBStorage(**FALKORDB_CONFIG)\n",
    "    \n",
    "    try:\n",
    "        # Get initial statistics\n",
    "        initial_stats = fdb.get_graph_stats()\n",
    "        print(f\"Initial graph stats: {initial_stats}\")\n",
    "        \n",
    "        # Optional: Clear existing data\n",
    "        clear_data = input(\"\\nClear existing graph data? (y/N): \")\n",
    "        if clear_data.lower() == 'y':\n",
    "            fdb.clear_graph()\n",
    "            print(\"Cleared existing graph data\")\n",
    "            initial_stats = {'nodes': 0, 'relationships': 0}\n",
    "        \n",
    "        # Store the knowledge graph in FalkorDB\n",
    "        print(\"\\nPushing knowledge graph to FalkorDB...\")\n",
    "        fdb.visualize_graph(knowledge_graph, parent_node_type=\"Article\")\n",
    "        \n",
    "        # Get final statistics\n",
    "        final_stats = fdb.get_graph_stats()\n",
    "        \n",
    "        # Calculate changes\n",
    "        nodes_added = final_stats['nodes'] - initial_stats['nodes']\n",
    "        rels_added = final_stats['relationships'] - initial_stats['relationships']\n",
    "        \n",
    "        print(f\"\\nGraph successfully stored in FalkorDB!\")\n",
    "        print(f\"Added: {nodes_added} nodes, {rels_added} relationships\")\n",
    "        print(f\"Total: {final_stats['nodes']} nodes, {final_stats['relationships']} relationships\")\n",
    "        \n",
    "        # Run sample analytics query\n",
    "        print(\"\\nðŸ” Running analytics query...\")\n",
    "        try:\n",
    "            sample_query = \"MATCH (n) RETURN labels(n) as entity_type, count(n) as count ORDER BY count DESC LIMIT 10\"\n",
    "            result = fdb.run_query_with_result(sample_query)\n",
    "            \n",
    "            print(\"Entity distribution:\")\n",
    "            if hasattr(result, 'result_set') and result.result_set:\n",
    "                for row in result.result_set:\n",
    "                    print(f\"  â€¢ {row[0]}: {row[1]} entities\")\n",
    "        except Exception as e:\n",
    "            print(f\"Analytics query failed: {e}\")\n",
    "        \n",
    "        return fdb\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to integrate with FalkorDB: {e}\")\n",
    "        raise\n",
    "\n",
    "# Integrate with FalkorDB\n",
    "falkor_storage = await integrate_with_falkordb(kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8789d31",
   "metadata": {},
   "source": [
    "## ðŸš€ Advanced FalkorDB Features\n",
    "\n",
    "Explore FalkorDB's powerful graph analytics capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bad008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Graph Analytics with FalkorDB\n",
    "print(\"FalkorDB Graph Analysis:\\n\")\n",
    "\n",
    "# Query 1: Find all relationship types and their frequencies\n",
    "try:\n",
    "    query1 = \"MATCH ()-[r]->() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC\"\n",
    "    result1 = falkor_storage.run_query_with_result(query1)\n",
    "    \n",
    "    print(\"Relationship types in your graph:\")\n",
    "    if hasattr(result1, 'result_set') and result1.result_set:\n",
    "        for row in result1.result_set[:5]:  # Show top 5\n",
    "            print(f\"  â€¢ {row[0]}: {row[1]} instances\")\n",
    "except Exception as e:\n",
    "    print(f\"Relationship query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Query 2: Find connection paths between key entities\n",
    "try:\n",
    "    query2 = \"MATCH path = (a)-[*1..2]-(b) WHERE a.name CONTAINS 'Apple' AND b.name CONTAINS 'Tim' RETURN length(path) as path_length, count(*) as paths\"\n",
    "    result2 = falkor_storage.run_query_with_result(query2)\n",
    "    \n",
    "    print(\"Connection paths between Apple and Tim:\")\n",
    "    if hasattr(result2, 'result_set') and result2.result_set:\n",
    "        for row in result2.result_set:\n",
    "            print(f\"  â€¢ Path length {row[0]}: {row[1]} paths\")\n",
    "    else:\n",
    "        print(\"  â€¢ No direct paths found\")\n",
    "except Exception as e:\n",
    "    print(f\"Path query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Query 3: Show sample nodes and their properties\n",
    "try:\n",
    "    query3 = \"MATCH (n) RETURN n.name, labels(n), n.description LIMIT 5\"\n",
    "    result3 = falkor_storage.run_query_with_result(query3)\n",
    "    \n",
    "    print(\"Sample entities in the graph:\")\n",
    "    if hasattr(result3, 'result_set') and result3.result_set:\n",
    "        for row in result3.result_set:\n",
    "            name = row[0] if row[0] else \"Unknown\"\n",
    "            labels = row[1] if row[1] else \"No label\"\n",
    "            desc = row[2] if len(row) > 2 and row[2] else \"No description\"\n",
    "            print(f\"  â€¢ {name} ({labels}): {desc[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Entity query failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Advanced analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b9319",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup\n",
    "\n",
    "Clean up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e598319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close FalkorDB connection\n",
    "try:\n",
    "    falkor_storage.close()\n",
    "    print(\"ðŸ”Œ FalkorDB connection closed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error closing connection: {e}\")\n",
    "\n",
    "print(\"\\nQuickstart completed successfully!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  â€¢ Try with your own documents\")\n",
    "print(\"  â€¢ Experiment with different schemas\")\n",
    "print(\"  â€¢ Explore advanced Cypher queries\")\n",
    "print(\"  â€¢ Build dynamic knowledge graphs with temporal data\")\n",
    "print(\"  â€¢ Scale up with larger document collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9c8a2",
   "metadata": {},
   "source": [
    "## ðŸ“š More Examples\n",
    "\n",
    "**Try different document types:**\n",
    "\n",
    "```python\n",
    "# Scientific articles\n",
    "from itext2kg.models.schemas import Article\n",
    "\n",
    "# CV/Resume processing  \n",
    "from itext2kg.models.schemas import CV\n",
    "\n",
    "# Custom schemas for your domain\n",
    "class YourCustomSchema(BaseModel):\n",
    "    # Define your fields here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Dynamic Knowledge Graphs:**\n",
    "```python\n",
    "# Build evolving graphs with temporal data\n",
    "kg = await itext2kg_star.build_graph(\n",
    "    sections=sections,\n",
    "    existing_knowledge_graph=previous_kg,  # Incremental updates\n",
    "    observation_date=\"2025-08-05\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047f927",
   "metadata": {},
   "source": [
    "## ðŸ”§ Troubleshooting\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "1. **FalkorDB Connection Error**: Ensure FalkorDB is running on the specified host/port\n",
    "2. **API Key Error**: Verify your OpenAI API key is set correctly\n",
    "3. **Import Error**: Make sure all packages are installed with correct versions\n",
    "4. **Extraction Failures**: The system includes fallback mechanisms for robustness\n",
    "\n",
    "**Resources:**\n",
    "- [iText2KG Documentation](https://github.com/auvalab/itext2kg)\n",
    "- [FalkorDB Documentation](https://www.falkordb.com/)\n",
    "- [FalkorDB Cypher Guide](https://docs.falkordb.com/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "\n",
    "**Support:**\n",
    "- GitHub Issues: [itext2kg/issues](https://github.com/auvalab/itext2kg/issues)\n",
    "- Community: [Discussions](https://github.com/auvalab/itext2kg/discussions)\n",
    "- FalkorDB Community: [FalkorDB Discord](https://discord.gg/falkordb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
